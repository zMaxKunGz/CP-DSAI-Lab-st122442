{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Modify the regression scratch code in our lecture such that:\n",
    "\n",
    "- Implement early stopping in which if the absolute difference between old loss and new loss does not exceed certain threshold, we abort the learning.\n",
    "\n",
    "- Implement options for stochastic gradient descent in which we use only one sample for training.  Make sure that sample does not repeat unless all samples are read at least once already.\n",
    "\n",
    "- Put everything into class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "boston = load_boston()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "boston\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "class RegressionModel:\n",
    "    \n",
    "    def __init__(self, max_iteration = 10000, alpha = 0.0001, loss_old = 10000, method=\"batch\",\n",
    "    tol = 1e-5) -> None:\n",
    "        self.max_iter = max_iteration\n",
    "        self.alpha = alpha\n",
    "        self.loss_old = loss_old\n",
    "        self.method = method\n",
    "        self.stop_iter = 0\n",
    "        self.tol = tol\n",
    "        self.mini_batch_size = 100\n",
    "\n",
    "    def closed_form(self, X, y):\n",
    "        self.theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        return self.theta\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.theta)\n",
    "\n",
    "    def mse(self, X, y, yhat):\n",
    "        return ((y - yhat)**2).sum() / X.shape[0]\n",
    "    \n",
    "    def h_theta(self, X, theta):\n",
    "        return X @ theta\n",
    "\n",
    "    def gradient(self, X, loss):\n",
    "        return X.T @ loss\n",
    "\n",
    "    def batch_gradient_descend(self, X, y):\n",
    "        theta = np.zeros(X.shape[1])\n",
    "        sto_used_index = []\n",
    "        for i in range(self.max_iter):\n",
    "            \n",
    "            if self.method == \"batch\":\n",
    "                X_train = X\n",
    "                y_train = y\n",
    "            elif self.method == \"mini-batch\":\n",
    "                index = np.random.randint(X.shape[0])\n",
    "                while index in sto_used_index:\n",
    "                    index = np.random.randint(X.shape[0])\n",
    "                X_train = X[index:index + self.mini_batch_size, :]\n",
    "                y_train = y[index:index + self.mini_batch_size]\n",
    "                sto_used_index.append(index)\n",
    "                if len(sto_used_index) == X.shape[0]:\n",
    "                    sto_used_index = []\n",
    "            elif self.method == \"stochastic\":\n",
    "                index = np.random.randint(X.shape[0])\n",
    "                while index in sto_used_index:\n",
    "                    index = np.random.randint(X.shape[0])\n",
    "                X_train = X[index, :].reshape(1, -1)\n",
    "                y_train = y[index]\n",
    "                sto_used_index.append(index)\n",
    "                if len(sto_used_index) == X.shape[0]:\n",
    "                    sto_used_index = []\n",
    "\n",
    "            yhat = self.h_theta(X_train, theta)\n",
    "            cost = yhat - y_train\n",
    "            grad = self.gradient(X_train, cost)\n",
    "            theta = theta - self.alpha * grad\n",
    "            \n",
    "            loss_new = self.mse(X_train, y_train, yhat)\n",
    "            if self.is_early_stop(loss_new, self.loss_old, self.tol):\n",
    "                self.stop_iter = i\n",
    "                break\n",
    "            else:\n",
    "                self.stop_iter = i\n",
    "                self.loss_old = loss_new\n",
    "            \n",
    "        self.theta = theta\n",
    "        return theta\n",
    "\n",
    "    def is_early_stop(self, loss_new, loss_old, tol) -> bool:\n",
    "        return np.abs(loss_new - loss_old) < tol\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "model = RegressionModel(method=\"batch\")\n",
    "theta = model.batch_gradient_descend(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(\"MSE: \", model.mse(X_test, y_test, yhat))\n",
    "print(\"Stop at Iteration:\", model.stop_iter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE:  17.867060332953567\n",
      "Stop at Iteration: 1150\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "model = RegressionModel(method=\"stochastic\")\n",
    "theta = model.batch_gradient_descend(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(\"MSE: \", model.mse(X_test, y_test, yhat))\n",
    "print(\"Stop at Iteration:\", model.stop_iter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE:  79.17960964416972\n",
      "Stop at Iteration: 9999\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "model = RegressionModel(method=\"mini-batch\", max_iteration=20000)\n",
    "theta = model.batch_gradient_descend(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(\"MSE: \", model.mse(X_test, y_test, yhat))\n",
    "print(\"Stop at Iteration:\", model.stop_iter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE:  18.08405967809017\n",
      "Stop at Iteration: 19999\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "interpreter": {
   "hash": "1df8770f36cccaa28b49186283820102a7be0be7912bde98ce292c19ce2372ee"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}